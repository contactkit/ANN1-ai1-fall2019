{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework.ipnyb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/contactkit/ANN1-ai1-fall2019/blob/master/Homework4_ipnyb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbaL-u2nV5qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqBoRRQcULcN",
        "colab_type": "code",
        "outputId": "82ae4469-f539-4445-cdb9-b53b9bc87b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n3Qbs2CXINq",
        "colab_type": "code",
        "outputId": "b50cb281-8c52-46d8-e50d-a5f2b777f6f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/23/b7b2c0dd59440abf27c7bcd6874e0ecd7de062848cfc132f1e5ba010e514/wandb-0.8.22-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/65/dca6d9653656630169e9f71fe3fcf2a487b671a096cf11dc3011c822a408/watchdog-0.10.1.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/5a/f1b0c63e40517b06bc21744a94013ca05de21de2687a59de889ea20a9ebd/sentry_sdk-0.14.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.9MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/8c/4543981439d23c4ff65b2e62dddd767ebc84a8e664a9b67e840d1e2730d3/GitPython-3.0.5-py3-none-any.whl (455kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 19.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.2MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/80/d7/2bfc9332e68d3e15ea97b9b1588b3899ad565120253d3fd71c8f7f13b4fe/shortuuid-0.5.0.tar.gz\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting gitdb2>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/6c/99296f89bad2ef85626e1df9f677acbee8885bb043ad82ad3ed4746d2325/gitdb2-2.0.6-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25hCollecting smmap2>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, gql, subprocess32, shortuuid, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.1-cp36-none-any.whl size=73503 sha256=71036edab5a47adebe6642ece0335c2e691c106aefc6c689919a24b2cbdcdce6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/07/ac/f8e4e29b1f6df6ed9569891d26a326f2af9ff8b77448cc46b8\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=371b930d82a3f9cc1c260b226c4306cc67f2b23ebd623e27ee6bc0833d94df8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=5e835720f83198b95127947f70eda4e7a883e6c3e670c8897aa1ed30a8be9c78\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shortuuid: filename=shortuuid-0.5.0-cp36-none-any.whl size=5499 sha256=8b59efd86302f144183ac545e22c928eac06093e5e150d57159c3c42e370cae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/eb/fd/69e5177f67b505e44acbd1aedfbe44b91768ee0c4cd5636576\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8786 sha256=396079675073dc08ade2344287be9b80e3a55f93b80d1d6cb5bbc414be5ad7e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104651 sha256=3aab142c5978efe68850bbce100770ba900cc9b09c9db0b5dc00ffcedb5e2fc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog gql subprocess32 shortuuid pathtools graphql-core\n",
            "Installing collected packages: pathtools, watchdog, graphql-core, gql, sentry-sdk, configparser, smmap2, gitdb2, GitPython, subprocess32, shortuuid, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.0.5 configparser-4.0.2 docker-pycreds-0.4.0 gitdb2-2.0.6 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.1 shortuuid-0.5.0 smmap2-2.0.5 subprocess32-3.5.4 wandb-0.8.22 watchdog-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvh_ksd9X3eL",
        "colab_type": "code",
        "outputId": "e3a77bcc-1bfa-4248-b7c1-8da6cb200475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 07ec05e4ac815f80c08a6ba0d5451a511a9084d9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a00AcP_fYaLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wandb.keras import WandbCallback\n",
        "import wandb\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbyuBpSkYwdk",
        "colab_type": "code",
        "outputId": "68d1cbb0-a863-4f00-dc4f-c2ecac187948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "run = wandb.init()\n",
        "config = run.config\n",
        "config.first_layer_convs = 64\n",
        "config.first_layer_conv_width = 5\n",
        "config.first_layer_conv_height = 5\n",
        "config.img_width = 32\n",
        "config.img_height = 32\n",
        "config.epochs = 20"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/univai-fall/uncategorized\" target=\"_blank\">https://app.wandb.ai/univai-fall/uncategorized</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/univai-fall/uncategorized/runs/unzel15s\" target=\"_blank\">https://app.wandb.ai/univai-fall/uncategorized/runs/unzel15s</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epv8olzRd6Vb",
        "colab_type": "code",
        "outputId": "c013d35d-d46c-4c1e-c222-9461745be4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP1lzaKheOTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_train /= 255.\n",
        "X_test = X_test.astype('float32')\n",
        "X_test /= 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scPW88A6eVp8",
        "colab_type": "code",
        "outputId": "e1f3607b-b0ac-4f55-a124-63f86b06c12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIJ6mhuqe36k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], config.img_width, config.img_height, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], config.img_width, config.img_height, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYBwh7M4fOau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "labels = range(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmSPBgjshPuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(64,(config.first_layer_conv_width, config.first_layer_conv_height),input_shape=(32,32,3),padding=\"same\",activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOry1R8Fitpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(192,(5,5),padding=\"same\",activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omou-xiQjmDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(384,(3,3),padding=\"same\",activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUcIp6OQq55M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(256,(5,5),padding=\"same\",activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S63HDxB4rGPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(256,(5,5),padding=\"same\",activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faUNHYQmwo1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAtWaMgUrWpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(256, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBb97gK0rzIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(4096, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogWArRUYr5di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epUG4lf3mnFj",
        "colab_type": "code",
        "outputId": "d748db71-d104-4146-a8ad-bb93a1c47c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        4864      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 192)       307392    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 384)         663936    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 256)         2457856   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 256)         1638656   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 6,232,138\n",
            "Trainable params: 6,232,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKa3lYGbv5ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLQiHsnzodXL",
        "colab_type": "code",
        "outputId": "57a20c28-9475-47ee-88a8-daed0ef5f4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=config.epochs,\n",
        "          callbacks=[WandbCallback(data_type=\"image\", save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "50000/50000 [==============================] - 45s 899us/step - loss: 1.7382 - acc: 0.3390 - val_loss: 1.4149 - val_acc: 0.4844\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 29s 590us/step - loss: 1.2481 - acc: 0.5477 - val_loss: 1.1307 - val_acc: 0.5929\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 30s 593us/step - loss: 1.0437 - acc: 0.6298 - val_loss: 1.0070 - val_acc: 0.6495\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 30s 594us/step - loss: 0.9048 - acc: 0.6839 - val_loss: 0.9665 - val_acc: 0.6717\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 30s 595us/step - loss: 0.7970 - acc: 0.7224 - val_loss: 0.9469 - val_acc: 0.6864\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 30s 595us/step - loss: 0.7019 - acc: 0.7588 - val_loss: 0.9438 - val_acc: 0.6860\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 30s 594us/step - loss: 0.6232 - acc: 0.7864 - val_loss: 0.9623 - val_acc: 0.6899\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 30s 596us/step - loss: 0.5489 - acc: 0.8114 - val_loss: 0.9733 - val_acc: 0.6865\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 30s 595us/step - loss: 0.4801 - acc: 0.8350 - val_loss: 1.0214 - val_acc: 0.6919\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 30s 597us/step - loss: 0.4238 - acc: 0.8541 - val_loss: 1.0704 - val_acc: 0.7015\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 30s 595us/step - loss: 0.3763 - acc: 0.8714 - val_loss: 1.1096 - val_acc: 0.6980\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 30s 595us/step - loss: 0.3335 - acc: 0.8858 - val_loss: 1.1061 - val_acc: 0.7008\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 30s 593us/step - loss: 0.3029 - acc: 0.8973 - val_loss: 1.1969 - val_acc: 0.6887\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 30s 594us/step - loss: 0.2672 - acc: 0.9101 - val_loss: 1.3527 - val_acc: 0.6837\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 30s 594us/step - loss: 0.2418 - acc: 0.9202 - val_loss: 1.3572 - val_acc: 0.6978\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 30s 594us/step - loss: 0.2263 - acc: 0.9247 - val_loss: 1.3754 - val_acc: 0.7010\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 30s 594us/step - loss: 0.2035 - acc: 0.9330 - val_loss: 1.4570 - val_acc: 0.6901\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 30s 592us/step - loss: 0.1967 - acc: 0.9351 - val_loss: 1.4020 - val_acc: 0.6993\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 30s 592us/step - loss: 0.1713 - acc: 0.9433 - val_loss: 1.4692 - val_acc: 0.6922\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 30s 597us/step - loss: 0.1633 - acc: 0.9456 - val_loss: 1.5842 - val_acc: 0.6978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f91a1f47ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF2XBXsJtTSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Conv2D(64,(config.first_layer_conv_width, config.first_layer_conv_height),input_shape=(32,32,3),padding=\"same\",activation='relu'))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJgJGfJvX-g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Conv2D(192,(5,5),padding=\"same\",activation='relu'))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVdUWA2gYJGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Conv2D(384,(3,3),padding=\"same\",activation='relu'))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTC8v5AvYOOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Conv2D(256,(5,5),padding=\"same\",activation='relu'))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwCfSx1uYTql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Conv2D(256,(5,5),padding=\"same\",activation='relu'))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxKvhaS8YZNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9jxG3nKYbev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Dense(256, activation='relu'))\n",
        "model1.add(Dropout(0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PEa7E-NYl1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Dense(4096, activation='relu'))\n",
        "model1.add(Dropout(0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD_7CIr_Yqbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo0CR4HAaKa0",
        "colab_type": "code",
        "outputId": "d9b4cf69-a404-4edd-ce83-102debfc38c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 32, 32, 64)        4864      \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 16, 16, 192)       307392    \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 8, 8, 384)         663936    \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 4, 4, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 4, 4, 256)         2457856   \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 2, 2, 256)         1638656   \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 6,232,138\n",
            "Trainable params: 6,232,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sFYm_1taSSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm4IGI56acUQ",
        "colab_type": "code",
        "outputId": "16c59e46-b5c3-4abb-f285-c67ef4fa9f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "model1.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=config.epochs,\n",
        "          callbacks=[WandbCallback(data_type=\"image\", save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "  256/50000 [..............................] - ETA: 4:39 - loss: 2.3189 - acc: 0.0898"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.102173). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 35s 697us/step - loss: 1.9207 - acc: 0.2620 - val_loss: 1.8704 - val_acc: 0.3315\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 34s 670us/step - loss: 1.5303 - acc: 0.4393 - val_loss: 1.4981 - val_acc: 0.4777\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 33s 666us/step - loss: 1.3999 - acc: 0.4975 - val_loss: 1.3913 - val_acc: 0.5170\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 33s 668us/step - loss: 1.3187 - acc: 0.5329 - val_loss: 1.3216 - val_acc: 0.5513\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 33s 667us/step - loss: 1.2602 - acc: 0.5575 - val_loss: 1.3529 - val_acc: 0.5464\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 33s 667us/step - loss: 1.2033 - acc: 0.5812 - val_loss: 1.2204 - val_acc: 0.5730\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 33s 667us/step - loss: 1.1724 - acc: 0.5940 - val_loss: 1.2442 - val_acc: 0.5975\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 33s 668us/step - loss: 1.1393 - acc: 0.6069 - val_loss: 1.2900 - val_acc: 0.5831\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 34s 671us/step - loss: 1.1250 - acc: 0.6143 - val_loss: 1.2096 - val_acc: 0.6026\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 33s 665us/step - loss: 1.0991 - acc: 0.6247 - val_loss: 1.1780 - val_acc: 0.6381\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 33s 668us/step - loss: 1.0764 - acc: 0.6343 - val_loss: 1.1963 - val_acc: 0.6282\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 33s 665us/step - loss: 1.0527 - acc: 0.6455 - val_loss: 1.1887 - val_acc: 0.6148\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 33s 664us/step - loss: 1.0424 - acc: 0.6498 - val_loss: 1.3033 - val_acc: 0.6045\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 33s 666us/step - loss: 1.0254 - acc: 0.6570 - val_loss: 1.1315 - val_acc: 0.6428\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 33s 665us/step - loss: 1.0183 - acc: 0.6618 - val_loss: 1.2904 - val_acc: 0.5898\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 33s 664us/step - loss: 0.9943 - acc: 0.6694 - val_loss: 1.1763 - val_acc: 0.6395\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 33s 664us/step - loss: 0.9962 - acc: 0.6694 - val_loss: 1.0943 - val_acc: 0.6636\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 33s 668us/step - loss: 0.9759 - acc: 0.6785 - val_loss: 1.1917 - val_acc: 0.6420\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 33s 666us/step - loss: 0.9589 - acc: 0.6813 - val_loss: 1.1412 - val_acc: 0.6403\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 33s 664us/step - loss: 0.9528 - acc: 0.6882 - val_loss: 1.1343 - val_acc: 0.6556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9163ac3f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGiuknW-aXU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(64,(config.first_layer_conv_width, config.first_layer_conv_height),input_shape=(32,32,3),padding=\"same\",activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uervstF8fc-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Conv2D(192,(5,5),padding=\"same\",activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2PnHkDGfdKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Conv2D(384,(3,3),padding=\"same\",activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzj6I49OfdTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Conv2D(256,(5,5),padding=\"same\",activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOMUMGL8fdZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Conv2D(256,(5,5),padding=\"same\",activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o1Vxac2f6Oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_8gl866f6S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNI6tTIcf6Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Dense(4096, activation='relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmo-AoRKgFml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL3SauGjgHXj",
        "colab_type": "code",
        "outputId": "795db7c7-ae60-4dc4-90c3-7345b0a4d093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_29 (Conv2D)           (None, 32, 32, 64)        4864      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 16, 16, 192)       307392    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 8, 8, 384)         663936    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 384)         1536      \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 4, 4, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 4, 4, 256)         2457856   \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 2, 2, 256)         1638656   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 6,254,154\n",
            "Trainable params: 6,243,146\n",
            "Non-trainable params: 11,008\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pe8ETP8hI1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffD3g4xohO0p",
        "colab_type": "code",
        "outputId": "57da20dd-3f79-447a-bab3-c445f9b1c89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=config.epochs,\n",
        "          callbacks=[WandbCallback(data_type=\"image\", save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "  256/50000 [..............................] - ETA: 9:28 - loss: 3.8560 - acc: 0.1289 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.107111). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 44s 871us/step - loss: 2.7341 - acc: 0.2843 - val_loss: 2.6333 - val_acc: 0.2082\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 41s 811us/step - loss: 2.2585 - acc: 0.3398 - val_loss: 2.5416 - val_acc: 0.2800\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 40s 802us/step - loss: 2.2667 - acc: 0.3514 - val_loss: 2.5256 - val_acc: 0.3130\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 2.3728 - acc: 0.3195 - val_loss: 4.2827 - val_acc: 0.2871\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 1.9632 - acc: 0.3379 - val_loss: 2.2403 - val_acc: 0.3579\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 1.5926 - acc: 0.4273 - val_loss: 2.4279 - val_acc: 0.3750\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 40s 803us/step - loss: 1.3919 - acc: 0.5036 - val_loss: 2.1306 - val_acc: 0.4555\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 40s 807us/step - loss: 1.2071 - acc: 0.5748 - val_loss: 1.7260 - val_acc: 0.5713\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 1.0553 - acc: 0.6357 - val_loss: 4.0442 - val_acc: 0.4719\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 40s 805us/step - loss: 0.9261 - acc: 0.6860 - val_loss: 1.9739 - val_acc: 0.4955\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.7983 - acc: 0.7315 - val_loss: 1.2551 - val_acc: 0.6741\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 40s 801us/step - loss: 0.7031 - acc: 0.7638 - val_loss: 1.2906 - val_acc: 0.6477\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 40s 801us/step - loss: 0.6151 - acc: 0.7961 - val_loss: 0.9640 - val_acc: 0.7159\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 40s 802us/step - loss: 0.5388 - acc: 0.8208 - val_loss: 1.0308 - val_acc: 0.7114\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 40s 801us/step - loss: 0.4643 - acc: 0.8460 - val_loss: 1.2084 - val_acc: 0.6742\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.4130 - acc: 0.8638 - val_loss: 1.0375 - val_acc: 0.7245\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 40s 806us/step - loss: 0.3651 - acc: 0.8815 - val_loss: 1.2920 - val_acc: 0.6842\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 40s 801us/step - loss: 0.3143 - acc: 0.8955 - val_loss: 1.1852 - val_acc: 0.7254\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 40s 801us/step - loss: 0.2818 - acc: 0.9058 - val_loss: 1.1429 - val_acc: 0.7336\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 40s 804us/step - loss: 0.2542 - acc: 0.9156 - val_loss: 1.0851 - val_acc: 0.7425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f916112e198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owz5ZKyjhUVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "model3 = Sequential()\n",
        "model3.add(Conv2D(64,(config.first_layer_conv_width, config.first_layer_conv_height),input_shape=(32,32,3),padding=\"same\",activation='relu',strides=2))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqWBqlZ3jWzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Conv2D(192,(5,5),padding=\"same\",activation='relu',strides=2))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMYxMt43jW3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Conv2D(384,(3,3),padding=\"same\",activation='relu',strides=2))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGK03D-njW7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Conv2D(256,(5,5),padding=\"same\",activation='relu',strides=2))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmdzcygmjW_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Conv2D(256,(5,5),padding=\"same\",activation='relu',strides=2))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CgGrHTqjXFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Conv2D(4096,(1,1),activation='relu',strides=1))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-c1Ou7kjXK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Conv2D(4096,(1,1), activation='relu',strides=1))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1STIvRsjXOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Conv2D(num_classes,(1,1), activation='softmax',strides=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RthhIaW9q2KC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-C63gUzqctZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAGu_oGfjXSC",
        "colab_type": "code",
        "outputId": "77bc1426-c0b8-4b29-ebbe-b0cd38cacc1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        4864      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 192)         307392    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 384)         663936    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4, 4, 384)         1536      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 2, 2, 256)         2457856   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 1, 1, 256)         1638656   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1, 1, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 1, 4096)        1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1, 1, 4096)        16384     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 4096)        16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 1, 1, 4096)        16384     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 10)          40970     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 22,985,144\n",
            "Trainable params: 22,966,456\n",
            "Non-trainable params: 18,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6HK0uG8m6Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OHZ3w7jnF_j",
        "colab_type": "code",
        "outputId": "8be3972a-91b6-4876-8934-0bcdc8e2f226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "source": [
        "model3.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=config.epochs,\n",
        "          callbacks=[WandbCallback(data_type=\"image\", save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 72s 1ms/step - loss: 2.2075 - acc: 0.1649 - val_loss: 2.2045 - val_acc: 0.1727\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.1532 - acc: 0.1772 - val_loss: 2.0985 - val_acc: 0.1931\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0964 - acc: 0.1833 - val_loss: 2.0833 - val_acc: 0.1970\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0794 - acc: 0.1886 - val_loss: 2.1704 - val_acc: 0.1826\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0726 - acc: 0.1902 - val_loss: 2.0516 - val_acc: 0.2153\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0520 - acc: 0.2030 - val_loss: 2.0571 - val_acc: 0.1996\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0443 - acc: 0.2075 - val_loss: 2.0052 - val_acc: 0.2181\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0398 - acc: 0.2126 - val_loss: 2.1342 - val_acc: 0.2040\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0303 - acc: 0.2179 - val_loss: 2.0100 - val_acc: 0.2217\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0408 - acc: 0.2130 - val_loss: 2.1875 - val_acc: 0.2054\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0385 - acc: 0.2190 - val_loss: 2.0218 - val_acc: 0.2088\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0881 - acc: 0.1991 - val_loss: 2.0575 - val_acc: 0.2014\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0241 - acc: 0.2121 - val_loss: 2.0142 - val_acc: 0.2003\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0595 - acc: 0.2009 - val_loss: 2.0147 - val_acc: 0.2021\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0303 - acc: 0.2055 - val_loss: 1.9919 - val_acc: 0.2082\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0146 - acc: 0.2110 - val_loss: 1.9985 - val_acc: 0.2047\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0128 - acc: 0.2106 - val_loss: 2.3141 - val_acc: 0.1576\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 64s 1ms/step - loss: 2.0503 - acc: 0.1951 - val_loss: 2.0118 - val_acc: 0.1907\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 63s 1ms/step - loss: 1.9991 - acc: 0.2116 - val_loss: 2.1395 - val_acc: 0.1839\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 63s 1ms/step - loss: 2.0261 - acc: 0.2070 - val_loss: 2.0098 - val_acc: 0.2256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84ec0a8208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Iu0YX9T5kb",
        "colab_type": "code",
        "outputId": "70415196-6f3a-4d94-93cb-72e2913ccb93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF1pCs4mlPUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy0BpzUIrJdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zr231vnrKOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Cfas9GrNR_",
        "colab_type": "code",
        "outputId": "1421eecb-faaf-4259-cba5-39b531d15488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model3.fit_generator(datagen.flow(x_train, y_train, batch_size=256),validation_data=(x_test,y_test),\n",
        "                    steps_per_epoch=len(x_train) /256, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "196/195 [==============================] - 31s 160ms/step - loss: 2.1542 - acc: 0.1898 - val_loss: 2.7415 - val_acc: 0.1001\n",
            "Epoch 2/20\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 2.0758 - acc: 0.2041 - val_loss: 2.6531 - val_acc: 0.1040\n",
            "Epoch 3/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 2.0468 - acc: 0.2083 - val_loss: 2.7150 - val_acc: 0.1003\n",
            "Epoch 4/20\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 2.0394 - acc: 0.2104 - val_loss: 2.7231 - val_acc: 0.1000\n",
            "Epoch 5/20\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 2.0315 - acc: 0.2115 - val_loss: 2.7403 - val_acc: 0.1000\n",
            "Epoch 6/20\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 2.0248 - acc: 0.2117 - val_loss: 2.7512 - val_acc: 0.1009\n",
            "Epoch 7/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 2.0258 - acc: 0.2165 - val_loss: 2.7609 - val_acc: 0.1016\n",
            "Epoch 8/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 2.0234 - acc: 0.2144 - val_loss: 2.7883 - val_acc: 0.1002\n",
            "Epoch 9/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 2.0061 - acc: 0.2147 - val_loss: 2.7973 - val_acc: 0.1005\n",
            "Epoch 10/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9950 - acc: 0.2191 - val_loss: 2.6880 - val_acc: 0.1118\n",
            "Epoch 11/20\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 2.0014 - acc: 0.2186 - val_loss: 2.1096 - val_acc: 0.1828\n",
            "Epoch 12/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9972 - acc: 0.2201 - val_loss: 2.6898 - val_acc: 0.1085\n",
            "Epoch 13/20\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9968 - acc: 0.2180 - val_loss: 2.4908 - val_acc: 0.1258\n",
            "Epoch 14/20\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 2.0004 - acc: 0.2174 - val_loss: 2.6801 - val_acc: 0.1103\n",
            "Epoch 15/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9914 - acc: 0.2239 - val_loss: 2.5959 - val_acc: 0.1212\n",
            "Epoch 16/20\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9929 - acc: 0.2245 - val_loss: 2.8830 - val_acc: 0.1002\n",
            "Epoch 17/20\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9887 - acc: 0.2197 - val_loss: 2.2884 - val_acc: 0.1622\n",
            "Epoch 18/20\n",
            "196/195 [==============================] - 28s 140ms/step - loss: 1.9851 - acc: 0.2212 - val_loss: 2.5378 - val_acc: 0.1257\n",
            "Epoch 19/20\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9943 - acc: 0.2195 - val_loss: 2.8212 - val_acc: 0.1019\n",
            "Epoch 20/20\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9975 - acc: 0.2161 - val_loss: 2.1999 - val_acc: 0.1577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8544055e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SQV_rM_sPrf",
        "colab_type": "code",
        "outputId": "fd977a55-74d3-4c44-aa21-514a4e009806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model3.fit_generator(datagen.flow(x_train, y_train, batch_size=256),validation_data=(x_test,y_test),\n",
        "                    steps_per_epoch=len(x_train) /256, epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "196/195 [==============================] - 28s 145ms/step - loss: 2.0043 - acc: 0.2192 - val_loss: 2.2994 - val_acc: 0.1586\n",
            "Epoch 2/200\n",
            "196/195 [==============================] - 28s 140ms/step - loss: 2.0060 - acc: 0.2207 - val_loss: 2.8085 - val_acc: 0.1000\n",
            "Epoch 3/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 2.0044 - acc: 0.2245 - val_loss: 2.8497 - val_acc: 0.1000\n",
            "Epoch 4/200\n",
            "196/195 [==============================] - 28s 140ms/step - loss: 1.9910 - acc: 0.2258 - val_loss: 2.8951 - val_acc: 0.1001\n",
            "Epoch 5/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9871 - acc: 0.2253 - val_loss: 2.2932 - val_acc: 0.1623\n",
            "Epoch 6/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9811 - acc: 0.2256 - val_loss: 2.8511 - val_acc: 0.1014\n",
            "Epoch 7/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9835 - acc: 0.2227 - val_loss: 2.8357 - val_acc: 0.1043\n",
            "Epoch 8/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9728 - acc: 0.2226 - val_loss: 2.9031 - val_acc: 0.1024\n",
            "Epoch 9/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9774 - acc: 0.2235 - val_loss: 2.9604 - val_acc: 0.1021\n",
            "Epoch 10/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9700 - acc: 0.2278 - val_loss: 2.9950 - val_acc: 0.1011\n",
            "Epoch 11/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9955 - acc: 0.2240 - val_loss: 3.0507 - val_acc: 0.1005\n",
            "Epoch 12/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9818 - acc: 0.2311 - val_loss: 3.1051 - val_acc: 0.1004\n",
            "Epoch 13/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9789 - acc: 0.2318 - val_loss: 3.1109 - val_acc: 0.1036\n",
            "Epoch 14/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9913 - acc: 0.2239 - val_loss: 3.1178 - val_acc: 0.1007\n",
            "Epoch 15/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9847 - acc: 0.2274 - val_loss: 2.8703 - val_acc: 0.1263\n",
            "Epoch 16/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9840 - acc: 0.2345 - val_loss: 3.1452 - val_acc: 0.1019\n",
            "Epoch 17/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9657 - acc: 0.2342 - val_loss: 2.5973 - val_acc: 0.1456\n",
            "Epoch 18/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9768 - acc: 0.2312 - val_loss: 2.9572 - val_acc: 0.1127\n",
            "Epoch 19/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9864 - acc: 0.2263 - val_loss: 2.2039 - val_acc: 0.1781\n",
            "Epoch 20/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9610 - acc: 0.2305 - val_loss: 3.0133 - val_acc: 0.1048\n",
            "Epoch 21/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9829 - acc: 0.2206 - val_loss: 2.4190 - val_acc: 0.1589\n",
            "Epoch 22/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9779 - acc: 0.2243 - val_loss: 2.8135 - val_acc: 0.1233\n",
            "Epoch 23/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9653 - acc: 0.2329 - val_loss: 3.0527 - val_acc: 0.1054\n",
            "Epoch 24/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9789 - acc: 0.2335 - val_loss: 2.9426 - val_acc: 0.1150\n",
            "Epoch 25/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9713 - acc: 0.2333 - val_loss: 2.4587 - val_acc: 0.1278\n",
            "Epoch 26/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9742 - acc: 0.2295 - val_loss: 2.9759 - val_acc: 0.1090\n",
            "Epoch 27/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9651 - acc: 0.2295 - val_loss: 2.6964 - val_acc: 0.1328\n",
            "Epoch 28/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9605 - acc: 0.2311 - val_loss: 2.7089 - val_acc: 0.1314\n",
            "Epoch 29/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9608 - acc: 0.2338 - val_loss: 2.9954 - val_acc: 0.1092\n",
            "Epoch 30/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9643 - acc: 0.2348 - val_loss: 3.0705 - val_acc: 0.1030\n",
            "Epoch 31/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9648 - acc: 0.2316 - val_loss: 2.3659 - val_acc: 0.1617\n",
            "Epoch 32/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9639 - acc: 0.2349 - val_loss: 3.0742 - val_acc: 0.1041\n",
            "Epoch 33/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9640 - acc: 0.2337 - val_loss: 2.4985 - val_acc: 0.1580\n",
            "Epoch 34/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9639 - acc: 0.2346 - val_loss: 2.9905 - val_acc: 0.1115\n",
            "Epoch 35/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9756 - acc: 0.2356 - val_loss: 3.0756 - val_acc: 0.1017\n",
            "Epoch 36/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9733 - acc: 0.2305 - val_loss: 2.1451 - val_acc: 0.1788\n",
            "Epoch 37/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9563 - acc: 0.2312 - val_loss: 2.1306 - val_acc: 0.1809\n",
            "Epoch 38/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9609 - acc: 0.2280 - val_loss: 2.7556 - val_acc: 0.1235\n",
            "Epoch 39/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9556 - acc: 0.2305 - val_loss: 3.0960 - val_acc: 0.1032\n",
            "Epoch 40/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9441 - acc: 0.2373 - val_loss: 2.7372 - val_acc: 0.1295\n",
            "Epoch 41/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9484 - acc: 0.2357 - val_loss: 2.6977 - val_acc: 0.1276\n",
            "Epoch 42/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9485 - acc: 0.2348 - val_loss: 2.3428 - val_acc: 0.1628\n",
            "Epoch 43/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9635 - acc: 0.2325 - val_loss: 3.0703 - val_acc: 0.1021\n",
            "Epoch 44/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9678 - acc: 0.2284 - val_loss: 3.0835 - val_acc: 0.1027\n",
            "Epoch 45/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9558 - acc: 0.2312 - val_loss: 3.0576 - val_acc: 0.1022\n",
            "Epoch 46/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9741 - acc: 0.2190 - val_loss: 2.9772 - val_acc: 0.1053\n",
            "Epoch 47/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9612 - acc: 0.2266 - val_loss: 2.9409 - val_acc: 0.1013\n",
            "Epoch 48/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9614 - acc: 0.2262 - val_loss: 2.8886 - val_acc: 0.1017\n",
            "Epoch 49/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9480 - acc: 0.2317 - val_loss: 2.8509 - val_acc: 0.1085\n",
            "Epoch 50/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9659 - acc: 0.2301 - val_loss: 2.4320 - val_acc: 0.1537\n",
            "Epoch 51/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9508 - acc: 0.2291 - val_loss: 2.9172 - val_acc: 0.1056\n",
            "Epoch 52/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9464 - acc: 0.2297 - val_loss: 2.9563 - val_acc: 0.1058\n",
            "Epoch 53/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9492 - acc: 0.2363 - val_loss: 2.9759 - val_acc: 0.1044\n",
            "Epoch 54/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9373 - acc: 0.2402 - val_loss: 2.8139 - val_acc: 0.1229\n",
            "Epoch 55/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9438 - acc: 0.2319 - val_loss: 2.8668 - val_acc: 0.1126\n",
            "Epoch 56/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9459 - acc: 0.2349 - val_loss: 2.7085 - val_acc: 0.1259\n",
            "Epoch 57/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9378 - acc: 0.2341 - val_loss: 2.9044 - val_acc: 0.1043\n",
            "Epoch 58/200\n",
            "196/195 [==============================] - 28s 140ms/step - loss: 1.9495 - acc: 0.2370 - val_loss: 2.9174 - val_acc: 0.1029\n",
            "Epoch 59/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9519 - acc: 0.2336 - val_loss: 2.6318 - val_acc: 0.1358\n",
            "Epoch 60/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9514 - acc: 0.2324 - val_loss: 2.9541 - val_acc: 0.1039\n",
            "Epoch 61/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9430 - acc: 0.2328 - val_loss: 2.9275 - val_acc: 0.1102\n",
            "Epoch 62/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9523 - acc: 0.2347 - val_loss: 2.4442 - val_acc: 0.1569\n",
            "Epoch 63/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9557 - acc: 0.2332 - val_loss: 2.9389 - val_acc: 0.1061\n",
            "Epoch 64/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9480 - acc: 0.2356 - val_loss: 2.4622 - val_acc: 0.1528\n",
            "Epoch 65/200\n",
            "196/195 [==============================] - 28s 143ms/step - loss: 1.9524 - acc: 0.2357 - val_loss: 2.3142 - val_acc: 0.1665\n",
            "Epoch 66/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9564 - acc: 0.2355 - val_loss: 2.4122 - val_acc: 0.1583\n",
            "Epoch 67/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9486 - acc: 0.2380 - val_loss: 2.2848 - val_acc: 0.1707\n",
            "Epoch 68/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9462 - acc: 0.2389 - val_loss: 2.8923 - val_acc: 0.1067\n",
            "Epoch 69/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9359 - acc: 0.2426 - val_loss: 2.7871 - val_acc: 0.1269\n",
            "Epoch 70/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9553 - acc: 0.2356 - val_loss: 2.8958 - val_acc: 0.1137\n",
            "Epoch 71/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9564 - acc: 0.2403 - val_loss: 2.7337 - val_acc: 0.1249\n",
            "Epoch 72/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9551 - acc: 0.2380 - val_loss: 2.5118 - val_acc: 0.1445\n",
            "Epoch 73/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9414 - acc: 0.2419 - val_loss: 2.7752 - val_acc: 0.1114\n",
            "Epoch 74/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9488 - acc: 0.2430 - val_loss: 2.6222 - val_acc: 0.1385\n",
            "Epoch 75/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9447 - acc: 0.2402 - val_loss: 2.8487 - val_acc: 0.1035\n",
            "Epoch 76/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9523 - acc: 0.2409 - val_loss: 2.8586 - val_acc: 0.1048\n",
            "Epoch 77/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9347 - acc: 0.2458 - val_loss: 2.6797 - val_acc: 0.1339\n",
            "Epoch 78/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9360 - acc: 0.2464 - val_loss: 2.8731 - val_acc: 0.1063\n",
            "Epoch 79/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9461 - acc: 0.2453 - val_loss: 2.5112 - val_acc: 0.1478\n",
            "Epoch 80/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9719 - acc: 0.2394 - val_loss: 2.7917 - val_acc: 0.1106\n",
            "Epoch 81/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9758 - acc: 0.2393 - val_loss: 2.4879 - val_acc: 0.1424\n",
            "Epoch 82/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9447 - acc: 0.2412 - val_loss: 2.2251 - val_acc: 0.1726\n",
            "Epoch 83/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9479 - acc: 0.2389 - val_loss: 2.8882 - val_acc: 0.1053\n",
            "Epoch 84/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9433 - acc: 0.2438 - val_loss: 2.5839 - val_acc: 0.1447\n",
            "Epoch 85/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9642 - acc: 0.2422 - val_loss: 2.4318 - val_acc: 0.1589\n",
            "Epoch 86/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9424 - acc: 0.2460 - val_loss: 2.2467 - val_acc: 0.1729\n",
            "Epoch 87/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9408 - acc: 0.2441 - val_loss: 2.4306 - val_acc: 0.1575\n",
            "Epoch 88/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9428 - acc: 0.2407 - val_loss: 2.4716 - val_acc: 0.1560\n",
            "Epoch 89/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9474 - acc: 0.2403 - val_loss: 2.5810 - val_acc: 0.1438\n",
            "Epoch 90/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9399 - acc: 0.2388 - val_loss: 2.6731 - val_acc: 0.1338\n",
            "Epoch 91/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9328 - acc: 0.2431 - val_loss: 2.7868 - val_acc: 0.1208\n",
            "Epoch 92/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9293 - acc: 0.2420 - val_loss: 2.9181 - val_acc: 0.1069\n",
            "Epoch 93/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9435 - acc: 0.2430 - val_loss: 2.5343 - val_acc: 0.1368\n",
            "Epoch 94/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9565 - acc: 0.2418 - val_loss: 2.7558 - val_acc: 0.1350\n",
            "Epoch 95/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9442 - acc: 0.2390 - val_loss: 2.1549 - val_acc: 0.1804\n",
            "Epoch 96/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9483 - acc: 0.2406 - val_loss: 2.5155 - val_acc: 0.1551\n",
            "Epoch 97/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9319 - acc: 0.2445 - val_loss: 2.3925 - val_acc: 0.1650\n",
            "Epoch 98/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9215 - acc: 0.2422 - val_loss: 2.4693 - val_acc: 0.1559\n",
            "Epoch 99/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9311 - acc: 0.2380 - val_loss: 2.7710 - val_acc: 0.1304\n",
            "Epoch 100/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9180 - acc: 0.2423 - val_loss: 2.3195 - val_acc: 0.1749\n",
            "Epoch 101/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9184 - acc: 0.2429 - val_loss: 2.4102 - val_acc: 0.1693\n",
            "Epoch 102/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9255 - acc: 0.2458 - val_loss: 2.1663 - val_acc: 0.1875\n",
            "Epoch 103/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9574 - acc: 0.2422 - val_loss: 2.0329 - val_acc: 0.1998\n",
            "Epoch 104/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9225 - acc: 0.2406 - val_loss: 2.7260 - val_acc: 0.1456\n",
            "Epoch 105/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9306 - acc: 0.2442 - val_loss: 2.7945 - val_acc: 0.1380\n",
            "Epoch 106/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9457 - acc: 0.2354 - val_loss: 2.2883 - val_acc: 0.1906\n",
            "Epoch 107/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9341 - acc: 0.2389 - val_loss: 2.5004 - val_acc: 0.1686\n",
            "Epoch 108/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9284 - acc: 0.2382 - val_loss: 2.3620 - val_acc: 0.1792\n",
            "Epoch 109/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9320 - acc: 0.2376 - val_loss: 2.6656 - val_acc: 0.1434\n",
            "Epoch 110/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9520 - acc: 0.2378 - val_loss: 2.9436 - val_acc: 0.1114\n",
            "Epoch 111/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9406 - acc: 0.2372 - val_loss: 3.0276 - val_acc: 0.1013\n",
            "Epoch 112/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9414 - acc: 0.2419 - val_loss: 2.7949 - val_acc: 0.1212\n",
            "Epoch 113/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9386 - acc: 0.2395 - val_loss: 2.1034 - val_acc: 0.1846\n",
            "Epoch 114/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9397 - acc: 0.2380 - val_loss: 2.3352 - val_acc: 0.1690\n",
            "Epoch 115/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9379 - acc: 0.2352 - val_loss: 2.3745 - val_acc: 0.1710\n",
            "Epoch 116/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9410 - acc: 0.2361 - val_loss: 2.2828 - val_acc: 0.1766\n",
            "Epoch 117/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9421 - acc: 0.2364 - val_loss: 2.1048 - val_acc: 0.1858\n",
            "Epoch 118/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9501 - acc: 0.2339 - val_loss: 2.2642 - val_acc: 0.1817\n",
            "Epoch 119/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9376 - acc: 0.2333 - val_loss: 2.1783 - val_acc: 0.1806\n",
            "Epoch 120/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9239 - acc: 0.2411 - val_loss: 2.1003 - val_acc: 0.1934\n",
            "Epoch 121/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9434 - acc: 0.2356 - val_loss: 2.2027 - val_acc: 0.1936\n",
            "Epoch 122/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9451 - acc: 0.2391 - val_loss: 2.5798 - val_acc: 0.1404\n",
            "Epoch 123/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9348 - acc: 0.2393 - val_loss: 2.6871 - val_acc: 0.1267\n",
            "Epoch 124/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9310 - acc: 0.2403 - val_loss: 2.5314 - val_acc: 0.1436\n",
            "Epoch 125/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9273 - acc: 0.2410 - val_loss: 2.2033 - val_acc: 0.1757\n",
            "Epoch 126/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9196 - acc: 0.2421 - val_loss: 2.3789 - val_acc: 0.1623\n",
            "Epoch 127/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9377 - acc: 0.2404 - val_loss: 2.1613 - val_acc: 0.1804\n",
            "Epoch 128/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9249 - acc: 0.2441 - val_loss: 2.1857 - val_acc: 0.1789\n",
            "Epoch 129/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9251 - acc: 0.2416 - val_loss: 2.6984 - val_acc: 0.1298\n",
            "Epoch 130/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9271 - acc: 0.2385 - val_loss: 2.6476 - val_acc: 0.1335\n",
            "Epoch 131/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9292 - acc: 0.2396 - val_loss: 2.7832 - val_acc: 0.1206\n",
            "Epoch 132/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9253 - acc: 0.2405 - val_loss: 2.1788 - val_acc: 0.1795\n",
            "Epoch 133/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9235 - acc: 0.2392 - val_loss: 2.3766 - val_acc: 0.1573\n",
            "Epoch 134/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9227 - acc: 0.2410 - val_loss: 2.3503 - val_acc: 0.1615\n",
            "Epoch 135/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9378 - acc: 0.2393 - val_loss: 2.4094 - val_acc: 0.1485\n",
            "Epoch 136/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9299 - acc: 0.2441 - val_loss: 2.2607 - val_acc: 0.1684\n",
            "Epoch 137/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9609 - acc: 0.2366 - val_loss: 2.2516 - val_acc: 0.1748\n",
            "Epoch 138/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9385 - acc: 0.2396 - val_loss: 2.5376 - val_acc: 0.1489\n",
            "Epoch 139/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9564 - acc: 0.2348 - val_loss: 2.3205 - val_acc: 0.1664\n",
            "Epoch 140/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9408 - acc: 0.2361 - val_loss: 2.2883 - val_acc: 0.1676\n",
            "Epoch 141/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9244 - acc: 0.2399 - val_loss: 2.8263 - val_acc: 0.1182\n",
            "Epoch 142/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9272 - acc: 0.2377 - val_loss: 2.5081 - val_acc: 0.1502\n",
            "Epoch 143/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9193 - acc: 0.2373 - val_loss: 2.3348 - val_acc: 0.1658\n",
            "Epoch 144/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9178 - acc: 0.2389 - val_loss: 2.6478 - val_acc: 0.1393\n",
            "Epoch 145/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9123 - acc: 0.2409 - val_loss: 2.6762 - val_acc: 0.1362\n",
            "Epoch 146/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9247 - acc: 0.2397 - val_loss: 2.9435 - val_acc: 0.1110\n",
            "Epoch 147/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9255 - acc: 0.2413 - val_loss: 2.9036 - val_acc: 0.1129\n",
            "Epoch 148/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9286 - acc: 0.2373 - val_loss: 2.8251 - val_acc: 0.1213\n",
            "Epoch 149/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9341 - acc: 0.2382 - val_loss: 2.1600 - val_acc: 0.1855\n",
            "Epoch 150/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9279 - acc: 0.2387 - val_loss: 2.4296 - val_acc: 0.1599\n",
            "Epoch 151/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9196 - acc: 0.2391 - val_loss: 2.9408 - val_acc: 0.1101\n",
            "Epoch 152/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9160 - acc: 0.2393 - val_loss: 2.8124 - val_acc: 0.1211\n",
            "Epoch 153/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9359 - acc: 0.2342 - val_loss: 2.6124 - val_acc: 0.1381\n",
            "Epoch 154/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9347 - acc: 0.2353 - val_loss: 2.6955 - val_acc: 0.1327\n",
            "Epoch 155/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9334 - acc: 0.2364 - val_loss: 2.9911 - val_acc: 0.1057\n",
            "Epoch 156/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9286 - acc: 0.2385 - val_loss: 2.9814 - val_acc: 0.1024\n",
            "Epoch 157/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9189 - acc: 0.2394 - val_loss: 2.8855 - val_acc: 0.1124\n",
            "Epoch 158/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9250 - acc: 0.2347 - val_loss: 3.0177 - val_acc: 0.1016\n",
            "Epoch 159/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9282 - acc: 0.2338 - val_loss: 2.9842 - val_acc: 0.1054\n",
            "Epoch 160/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9234 - acc: 0.2367 - val_loss: 2.7491 - val_acc: 0.1239\n",
            "Epoch 161/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9305 - acc: 0.2317 - val_loss: 3.0758 - val_acc: 0.1006\n",
            "Epoch 162/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9325 - acc: 0.2324 - val_loss: 3.1314 - val_acc: 0.1005\n",
            "Epoch 163/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9259 - acc: 0.2342 - val_loss: 2.7800 - val_acc: 0.1266\n",
            "Epoch 164/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9264 - acc: 0.2336 - val_loss: 3.1099 - val_acc: 0.1043\n",
            "Epoch 165/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9220 - acc: 0.2391 - val_loss: 3.0400 - val_acc: 0.1088\n",
            "Epoch 166/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9219 - acc: 0.2435 - val_loss: 2.9664 - val_acc: 0.1118\n",
            "Epoch 167/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9517 - acc: 0.2365 - val_loss: 3.0836 - val_acc: 0.1004\n",
            "Epoch 168/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9337 - acc: 0.2380 - val_loss: 2.9117 - val_acc: 0.1160\n",
            "Epoch 169/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9349 - acc: 0.2340 - val_loss: 2.5788 - val_acc: 0.1468\n",
            "Epoch 170/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9160 - acc: 0.2342 - val_loss: 2.8218 - val_acc: 0.1228\n",
            "Epoch 171/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9174 - acc: 0.2351 - val_loss: 3.0259 - val_acc: 0.1079\n",
            "Epoch 172/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9074 - acc: 0.2398 - val_loss: 3.1034 - val_acc: 0.1048\n",
            "Epoch 173/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9069 - acc: 0.2344 - val_loss: 2.9374 - val_acc: 0.1166\n",
            "Epoch 174/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9086 - acc: 0.2365 - val_loss: 2.9909 - val_acc: 0.1104\n",
            "Epoch 175/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9174 - acc: 0.2323 - val_loss: 3.0522 - val_acc: 0.1073\n",
            "Epoch 176/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9105 - acc: 0.2358 - val_loss: 3.0215 - val_acc: 0.1095\n",
            "Epoch 177/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9080 - acc: 0.2375 - val_loss: 2.8399 - val_acc: 0.1262\n",
            "Epoch 178/200\n",
            "196/195 [==============================] - 28s 141ms/step - loss: 1.9085 - acc: 0.2366 - val_loss: 2.9115 - val_acc: 0.1210\n",
            "Epoch 179/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9068 - acc: 0.2403 - val_loss: 2.7397 - val_acc: 0.1375\n",
            "Epoch 180/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9098 - acc: 0.2404 - val_loss: 2.1207 - val_acc: 0.1834\n",
            "Epoch 181/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9227 - acc: 0.2341 - val_loss: 2.4010 - val_acc: 0.1624\n",
            "Epoch 182/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9352 - acc: 0.2399 - val_loss: 2.3200 - val_acc: 0.1576\n",
            "Epoch 183/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9443 - acc: 0.2390 - val_loss: 2.1242 - val_acc: 0.1792\n",
            "Epoch 184/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9145 - acc: 0.2412 - val_loss: 2.6549 - val_acc: 0.1372\n",
            "Epoch 185/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9082 - acc: 0.2433 - val_loss: 2.7178 - val_acc: 0.1346\n",
            "Epoch 186/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9165 - acc: 0.2396 - val_loss: 3.0049 - val_acc: 0.1081\n",
            "Epoch 187/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9272 - acc: 0.2408 - val_loss: 2.9319 - val_acc: 0.1153\n",
            "Epoch 188/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9092 - acc: 0.2429 - val_loss: 3.0631 - val_acc: 0.1069\n",
            "Epoch 189/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9151 - acc: 0.2399 - val_loss: 2.7879 - val_acc: 0.1319\n",
            "Epoch 190/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9140 - acc: 0.2415 - val_loss: 3.1214 - val_acc: 0.1045\n",
            "Epoch 191/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9104 - acc: 0.2400 - val_loss: 3.0545 - val_acc: 0.1132\n",
            "Epoch 192/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9104 - acc: 0.2406 - val_loss: 3.0502 - val_acc: 0.1096\n",
            "Epoch 193/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9135 - acc: 0.2377 - val_loss: 3.0563 - val_acc: 0.1105\n",
            "Epoch 194/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9223 - acc: 0.2365 - val_loss: 3.1741 - val_acc: 0.1005\n",
            "Epoch 195/200\n",
            "196/195 [==============================] - 27s 140ms/step - loss: 1.9262 - acc: 0.2315 - val_loss: 2.7602 - val_acc: 0.1419\n",
            "Epoch 196/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9120 - acc: 0.2365 - val_loss: 2.1247 - val_acc: 0.1822\n",
            "Epoch 197/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9200 - acc: 0.2332 - val_loss: 2.1627 - val_acc: 0.1818\n",
            "Epoch 198/200\n",
            "196/195 [==============================] - 27s 138ms/step - loss: 1.9247 - acc: 0.2380 - val_loss: 3.0421 - val_acc: 0.1100\n",
            "Epoch 199/200\n",
            "196/195 [==============================] - 27s 139ms/step - loss: 1.9173 - acc: 0.2384 - val_loss: 2.7226 - val_acc: 0.1361\n",
            "Epoch 200/200\n",
            "196/195 [==============================] - 27s 137ms/step - loss: 1.9071 - acc: 0.2395 - val_loss: 2.2052 - val_acc: 0.1760\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84e2687a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBK1jIuZyn4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}